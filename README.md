# American Sign Language Recognition System 

A deep learning-based system for recognizing American Sign Language (ASL) gestures using a combination of **Convolutional Neural Networks (CNN)** for Alphabets and numbers, **Multi-Layer Perceptrons (MLP)** for static two-hand gestures, and **Long Short-Term Memory (LSTM)** networks for dynamic sign recognition.

## Project Overview

This project aims to create an intelligent system capable of recognizing both static and dynamic ASL gestures. It consists of three core modules:

- ğŸ–¼ï¸ **CNN Model** â€” Recognizes static signs (A-Z, 0â€“9) from cropped hand images.
- ğŸ§  **MLP Model** â€” Classifies static two-hand signs based on landmark coordinates.
- ğŸ¥ **LSTM Model** â€” Detects dynamic signs using time-sequenced landmark data.

 ## Directions

 Each folder contains data collection, pre-processing, training and testing files for each type of model: CNN, MLP, and LSTM



